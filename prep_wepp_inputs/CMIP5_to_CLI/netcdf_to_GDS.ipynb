{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Garner\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5168: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n",
      "C:\\Users\\Garner\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "def netcdf_to_GDS(netcdf_dir, var_cols, proj_names, num_locs, proj_num, dwnsc_type, GDS_out_path):\n",
    "    '''\n",
    "    Extracts netcdf climate data from .nc files to workable dataframes \n",
    "    and then converts dataframes to GDS format\n",
    "\n",
    "    netcdf_dir = path to directory that has extraction_\"var\".nc files\n",
    "    var_cols = column names of variables (bcca and loca have different names)\n",
    "    proj_names = path to text file with climate projection names\n",
    "    num_locs = number of climate locations (modeled areas)\n",
    "    proj_num = number of projections/models in netcdf\n",
    "    '''\n",
    "    \n",
    "    def netCDF_to_dic():\n",
    "        '''\n",
    "        Extracts netCDF data to dictionary of dataframes, creates necessary edits\n",
    "        and adjustments to dataframes, and then exports to dataframes to text files\n",
    "        '''\n",
    "        \n",
    "        os.chdir(netcdf_dir)\n",
    "        \n",
    "        ## Load netCDF files into a list\n",
    "        nc_files = [x for x in os.listdir('.') if x.endswith('.nc')]\n",
    "\n",
    "        ## Create a list of lists where each sub-list contains the netCDF\n",
    "        ## files for pr, tmax, and tasmin\n",
    "        def divide_list(d,n):\n",
    "            '''\n",
    "            divide netCDF file list into groups of \"n\"\n",
    "            '''\n",
    "            for i in range(0, len(d), n):\n",
    "                yield d[i:i + n]\n",
    "\n",
    "        nc_sep_list = list(divide_list(nc_files,3))\n",
    "\n",
    "\n",
    "        ## Convert netCDF files to dataframes\n",
    "        netcdf_dic = {}\n",
    "        for lst in nc_sep_list:\n",
    "            dic_name = str(lst[0][0:5])\n",
    "            netcdf_dic[dic_name] = {}\n",
    "            for nc in lst:\n",
    "                ds = xr.open_dataset(nc)\n",
    "                netcdf_dic[dic_name][str(nc[9::])] = ds.to_dataframe()\n",
    "\n",
    "\n",
    "        ## Merge variables into one dataframe\n",
    "        merged_dic = {} \n",
    "\n",
    "        def merge_cols(d, new_dic):\n",
    "            '''\n",
    "            Merge the pr, tmax, and tmin dataframes together into dictionary\n",
    "            '''\n",
    "            for dic in d:\n",
    "                joined = pd.merge(d[dic]['pr.nc'], d[dic]['tasmax.nc'], on = var_cols)\n",
    "                new_dic[dic] = pd.merge(joined, d[dic]['tasmin.nc'], on = var_cols)\n",
    "\n",
    "        merge_cols(netcdf_dic, merged_dic)\n",
    "\n",
    "\n",
    "        def index_to_col(d):\n",
    "            '''\n",
    "            Make lat, lon, proj, and time indices into columns\n",
    "            cols = column key names for downscaling method\n",
    "            '''\n",
    "            for key in d:\n",
    "                d[key] = d[key].reset_index(level=var_cols)\n",
    "\n",
    "        index_to_col(merged_dic)\n",
    "\n",
    "\n",
    "        ## Convert date/time columns into GDS format\n",
    "        def edit_date_col(d):\n",
    "            '''\n",
    "            convert date/time columns into GDS format\n",
    "            '''\n",
    "            for key in d:\n",
    "                d[key].time = d[key].time.astype(str).str[2:10]\n",
    "                d[key].time = d[key].time.astype(str).replace('(-)', '', regex=True)\n",
    "\n",
    "        edit_date_col(merged_dic)\n",
    "\n",
    "\n",
    "        if proj_num > 1:\n",
    "            ## Split site dataframes into separate dataframes   \n",
    "            proj_dict = {}\n",
    "\n",
    "            def split_proj(d, new_dic):\n",
    "                '''\n",
    "                Split site dataframes into separate dataframes by projection and assign them to\n",
    "                a new dictionary (Dictionary order is: LOCA->Site->projection)\n",
    "                '''\n",
    "                for df in d:\n",
    "                    new_dic[df] = {k: v for k, v in d[df].groupby('projection')}\n",
    "\n",
    "            split_proj(merged_dic, proj_dict)\n",
    "\n",
    "\n",
    "            ## Rename dictionary keys and projection columns\n",
    "            renamed_dic = {}\n",
    "\n",
    "            def rename_keys(names, dic, new_dic):\n",
    "                '''\n",
    "                rename dictionary keys and projection columns\n",
    "                '''\n",
    "                for df in dic:\n",
    "                    new_dic[df] = dict(zip(names, list(dic[df].values())))\n",
    "\n",
    "            proj_list = open(proj_names, 'r')\n",
    "            proj_name_list = [line.rstrip('\\n') for line in proj_list.readlines()]\n",
    "\n",
    "            ### Run rename_keys for each dictionary\n",
    "            rename_keys(proj_name_list, proj_dict, renamed_dic)\n",
    "\n",
    "\n",
    "        ## Split dataframes by location if there are multiple locations\n",
    "        if num_locs > 1:\n",
    "            sep_loc_dic = {}\n",
    "\n",
    "            def sep_by_loc(d, new_dic, end):\n",
    "                '''\n",
    "                seperate the LOCA and BCCA dataframes by locations\n",
    "                '''\n",
    "                for key in d:\n",
    "                    for proj in d[key]:\n",
    "                        grouped_df = d[key][proj].groupby([var_cols[1], var_cols[0]])\n",
    "\n",
    "                        for (new_key,item),i in zip(grouped_df, range(1,end)):\n",
    "                            new_dic[str(key)[0:3] + '_' + proj + '_' + str(i)] = grouped_df.get_group(new_key)\n",
    "\n",
    "\n",
    "            sep_by_loc(renamed_dic, sep_loc_dic, num_locs) \n",
    "\n",
    "        if proj_num > 1 and num_locs >1:\n",
    "            output_dic = sep_loc_dic\n",
    "\n",
    "        if proj_num > 1 and num_locs <=1:\n",
    "            output_dic = renamed_dic\n",
    "\n",
    "        if proj_num == 1 and num_locs == 1:\n",
    "            output_dic = merged_dic\n",
    "\n",
    "        return output_dic\n",
    "\n",
    "    #Create dictionaries of workable dataframes for each downscaling method\n",
    "    prep_dic = netCDF_to_dic()\n",
    "\n",
    "    #Reset index key for easier indexing in future functions\n",
    "    for key in prep_dic:\n",
    "        prep_dic[key].reset_index(inplace = True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def to_GDS_file(input_dic, data_type):\n",
    "        '''\n",
    "        Formats dataframes in prep_dic to GDS format and downloads them to \n",
    "        text files\n",
    "        '''\n",
    "        if data_type == 'mod':\n",
    "\n",
    "            def sep_by_period(dic, new_dic):\n",
    "                '''\n",
    "                seperate dataframes by time periods\n",
    "                '''\n",
    "                for key in dic:\n",
    "                    new_dic[key + '_19'] = dic[key].loc[0:20087]\n",
    "                    new_dic[key + '_59'] = dic[key].loc[20088:34697]\n",
    "                    new_dic[key + '_99'] = dic[key].loc[34698:49308]\n",
    "\n",
    "            sep_dic = {}\n",
    "            sep_by_period(input_dic, sep_dic)\n",
    "\n",
    "        if data_type == 'obs':\n",
    "            sep_dic = input_dic\n",
    "\n",
    "\n",
    "        if dwnsc_type == 'BCCA' and data_type == 'mod':\n",
    "            ### rename lon and lat columns in BCCA to match with LOCA columns\n",
    "            for df in sep_dic:\n",
    "                sep_dic[df] = sep_dic[df].rename(columns={'longitude': 'lon'})\n",
    "                sep_dic[df] = sep_dic[df].rename(columns={'latitude' : 'lat'})\n",
    "\n",
    "\n",
    "        def coord_360to180(d):\n",
    "            '''\n",
    "            Convert longitude values from 360 degree format to 180 degree format\n",
    "            and remove \"-\" in front of the value\n",
    "            '''\n",
    "            for key in d:\n",
    "                if data_type == 'mod':\n",
    "                    d[key].lon = d[key].lon - 360\n",
    "                    d[key].lon = d[key].lon.astype(str).replace('(-)', '', regex=True)\n",
    "                    d[key].lon = d[key].lon.astype(float)\n",
    "\n",
    "                if data_type == 'obs':\n",
    "                    d[key].lon = d[key].lon.astype(str).replace('(-)', '', regex=True)\n",
    "                    d[key].lon = d[key].lon.astype(float)\n",
    "\n",
    "        coord_360to180(sep_dic)\n",
    "\n",
    "        def dd_to_dms(dd):\n",
    "            '''\n",
    "            Convert degree decimal coordinates to DMS format\n",
    "\n",
    "            Function is used in coord_to_dict function\n",
    "            '''\n",
    "            mnt,sec = divmod(dd*3600,60)\n",
    "            deg, mnt = divmod(mnt,60)\n",
    "\n",
    "            if mnt >= 10:\n",
    "                    deg = str('0' +str(deg)[:2])\n",
    "                    mnt = str(mnt)[:2]\n",
    "                    return deg+mnt\n",
    "\n",
    "            elif mnt <10:\n",
    "                    deg = str('0' +str(deg)[:2])\n",
    "                    mnt = str('0' +str(mnt)[:1])\n",
    "                    return deg+mnt\n",
    "\n",
    "\n",
    "        def coord_to_dict(d, new_lat, new_lon):\n",
    "            '''\n",
    "            Create a dictionary of lat/lon values for each lat/lon combo\n",
    "            '''\n",
    "            for key in d:\n",
    "                new_lat[key] = dd_to_dms(d[key].lat.iloc[1])\n",
    "            for key in d:\n",
    "                new_lon[key] = dd_to_dms(d[key].lon.iloc[1])\n",
    "\n",
    "        ## Create empty dictionaries for new lat and lon values\n",
    "        lat_values = {}\n",
    "        lon_values = {}        \n",
    "\n",
    "        coord_to_dict(sep_dic, lat_values, lon_values)\n",
    "\n",
    "\n",
    "        ### Drop unneeded columns\n",
    "        columns_drop = ['projection', 'lat', 'lon']\n",
    "        for df in sep_dic:\n",
    "            sep_dic[df].drop(columns_drop, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        def reorder_col(d):\n",
    "            '''\n",
    "            Reorder columns to fit GDS format\n",
    "            '''\n",
    "            order = ['time', 'tasmax', 'tasmin', 'pr']\n",
    "            for key in d:\n",
    "                d[key] = d[key].reindex(columns = order)\n",
    "\n",
    "        reorder_col(sep_dic)\n",
    "\n",
    "\n",
    "        def round_vals(d):\n",
    "            '''\n",
    "            Round values in tmax, tmin, and pr columns to 2 decimal places\n",
    "            '''\n",
    "            for key in d:\n",
    "                d[key] = d[key].round(decimals=1)\n",
    "\n",
    "        round_vals(sep_dic)\n",
    "\n",
    "        def remove_low_pr(d):\n",
    "            for key in d:\n",
    "                d[key]['pr'] = d[key]['pr'].replace(0.1, 0)\n",
    "                d[key]['pr'] = d[key]['pr'].replace(0.2, 0)\n",
    "                d[key]['pr'] = d[key]['pr'].replace(0.3, 0)\n",
    "\n",
    "\n",
    "        remove_low_pr(sep_dic)\n",
    "\n",
    "        ## Create empy dictionaries for ID strings\n",
    "        ID_strings = {}\n",
    "\n",
    "        ## Create dictionary of elevation values by site\n",
    "        elev_dic = {'BE1':'299','DO1':'388', 'GO1':'336',\\\n",
    "                    'RO1':'462', 'ST1':'402'}\n",
    "\n",
    "        ## ID strings at top of GDS files require specific number of spaces\n",
    "        ## between country/site ID and lat/lon/elevation values\n",
    "        if data_type == 'mod':\n",
    "            spaces = str('                                   ')\n",
    "        if data_type == 'obs':\n",
    "            spaces = str('                                         ')\n",
    "\n",
    "        def Create_top_info(d, new_dic, lat_vals, lon_vals):\n",
    "            '''\n",
    "            Create string with site, file name, lat, lon, and elev IDs\n",
    "            and assign to keys in dictionary\n",
    "            '''\n",
    "            for key in d:\n",
    "                for k in elev_dic:\n",
    "                    if key.startswith(k):\n",
    "                        elev = elev_dic[k]\n",
    "                        new_dic[key] = (str('99048') + key + spaces + lat_vals[key] + str('  ') + lon_vals[key] + elev)\n",
    "\n",
    "        Create_top_info(sep_dic,ID_strings, lat_values, lon_values)\n",
    "\n",
    "\n",
    "        ### Create new file for each dataframe and write ID strings and data in GDS\n",
    "        ### to it\n",
    "        for df, ID in zip(sep_dic, ID_strings):\n",
    "            with open(str(GDS_out_path + df + '.txt'), 'w+') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "                df = sep_dic[df]\n",
    "\n",
    "                new_lines = ['{}{}  {}  {}'.format(date, str(tmax)[0:5], str(tmin)[0:5], str(pr)[0:5]) \\\n",
    "                                                   for date, tmax, tmin, pr in \\\n",
    "                                                   zip(df['time'], df['tasmax'], df['tasmin'], df['pr'])]\n",
    "\n",
    "                file.writelines(str(ID_strings[ID])+'\\n')\n",
    "                for new_line in new_lines:\n",
    "                    file.writelines(str(new_line)+'\\n')\n",
    "\n",
    "    #Run to_GDS_file function                \n",
    "    to_GDS_file(prep_dic, 'mod')\n",
    "    \n",
    "#Set up input parameters for netcdf_to_GDS function and run for each downscaling type\n",
    "LOCA_netcdf = 'C:\\\\Users\\\\Garner\\\\Soil_Erosion_Project\\\\WEPP_PRWs\\\\GO1_DEP\\\\netcdf\\\\LOCA\\\\'\n",
    "BCCA_netcdf ='C:\\\\Users\\\\Garner\\\\Soil_Erosion_Project\\\\WEPP_PRWs\\\\GO1_DEP\\\\netcdf\\\\BCCA\\\\'\n",
    "\n",
    "LOCA_cols = ['lat', 'lon', 'projection', 'time']\n",
    "BCCA_cols = ['latitude', 'longitude', 'projection', 'time']\n",
    "\n",
    "LOCA_projs = 'C:\\\\Users\\\\Garner\\\\Soil_Erosion_Project\\\\WEPP_PRWs\\\\GO1_DEP\\\\netcdf\\\\LOCA_projections_Short.txt'\n",
    "BCCA_projs = 'C:\\\\Users\\\\Garner\\\\Soil_Erosion_Project\\\\WEPP_PRWs\\\\GO1_DEP\\\\netcdf\\\\BCCA_projections_Short.txt'\n",
    "\n",
    "LOCA_GDS_path = 'C:\\\\Users\\\\Garner\\\\Soil_Erosion_Project\\\\WEPP_PRWs\\\\GO1_DEP\\\\GDS\\\\test_outputs\\\\'     \n",
    "BCCA_GDS_path = 'C:\\\\Users\\\\Garner\\\\Soil_Erosion_Project\\\\WEPP_PRWs\\\\GO1_DEP\\\\GDS\\\\test_outputs\\\\' \n",
    "\n",
    "\n",
    "netcdf_to_GDS(LOCA_netcdf, LOCA_cols, LOCA_projs, 9, 6, 'LOCA', LOCA_GDS_path)\n",
    "netcdf_to_GDS(BCCA_netcdf, BCCA_cols, BCCA_projs, 3, 6, 'BCCA', BCCA_GDS_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
